import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import matplotlib
from torch.utils.data import DataLoader, Dataset
from Unet import UNet
import copy
from tqdm import tqdm
import os
import torchvision.transforms as transforms
from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts
from scipy.ndimage import gaussian_filter, map_coordinates
import cv2
import math

# è®¾ç½®matplotlibä¸­æ–‡å­—ä½“ï¼Œè§£å†³ä¸­æ–‡ä¹±ç é—®é¢˜
import platform
import warnings
import matplotlib.font_manager as fm

# æŠ‘åˆ¶æ‰€æœ‰matplotlibç›¸å…³çš„è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib.font_manager')
warnings.filterwarnings('ignore', message='.*does not have a glyph.*')
warnings.filterwarnings('ignore', message='.*substituting with a dummy symbol.*')

# æŠ‘åˆ¶å­—ä½“è­¦å‘Š
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

# å°è¯•è®¾ç½®ä¸­æ–‡å­—ä½“ï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨è‹±æ–‡
try:
    # Windowsç³»ç»Ÿå­—ä½“è®¾ç½®
    if platform.system() == 'Windows':
        matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
    elif platform.system() == 'Darwin':  # macOS
        matplotlib.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Hiragino Sans GB']
    else:  # Linux
        matplotlib.rcParams['font.sans-serif'] = ['WenQuanYi Micro Hei', 'DejaVu Sans']
    
    # å…³é”®ï¼šå¼ºåˆ¶ç¦ç”¨unicodeè´Ÿå·ï¼Œä½¿ç”¨ASCIIè´Ÿå·
    matplotlib.rcParams['axes.unicode_minus'] = False
    matplotlib.rcParams['font.size'] = 10
    
    # é¢å¤–è®¾ç½®ä»¥é¿å…å­—ä½“è­¦å‘Š
    matplotlib.rcParams['font.family'] = 'sans-serif'
    matplotlib.rcParams['mathtext.fontset'] = 'dejavusans'  # æ•°å­¦æ–‡æœ¬å­—ä½“
    matplotlib.rcParams['mathtext.default'] = 'regular'     # æ•°å­¦æ–‡æœ¬æ ·å¼
    
    # æµ‹è¯•ä¸­æ–‡å­—ä½“æ˜¯å¦å¯ç”¨
    import matplotlib.pyplot as plt
    fig, ax = plt.subplots()
    ax.text(0.5, 0.5, 'æµ‹è¯•', fontsize=12)
    plt.close(fig)
    
    USE_CHINESE = True
    print("âœ“ ä¸­æ–‡å­—ä½“è®¾ç½®æˆåŠŸ")
    
except Exception as e:
    # å¦‚æœä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial']
    matplotlib.rcParams['axes.unicode_minus'] = False
    matplotlib.rcParams['mathtext.fontset'] = 'dejavusans'
    matplotlib.rcParams['mathtext.default'] = 'regular'
    USE_CHINESE = False
    print("âš  ä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")

# é¢å¤–çš„è­¦å‘ŠæŠ‘åˆ¶
import logging
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)
logging.getLogger('matplotlib').setLevel(logging.ERROR)

# å¼ºåˆ¶é‡å»ºå­—ä½“ç¼“å­˜
try:
    fm._rebuild()
except:
    pass

# æ›´å¼ºåˆ¶æ€§çš„ä¸­æ–‡å­—ä½“è®¾ç½®
USE_CHINESE = False

try:
    # æ¸…é™¤ç°æœ‰å­—ä½“è®¾ç½®
    matplotlib.rcdefaults()
    
    # æ ¹æ®ç³»ç»Ÿè®¾ç½®å­—ä½“
    if platform.system() == 'Windows':
        # Windowsç³»ç»Ÿå¸¸è§ä¸­æ–‡å­—ä½“
        font_candidates = ['SimHei', 'Microsoft YaHei', 'SimSun', 'KaiTi', 'FangSong']
    elif platform.system() == 'Darwin':  # macOS
        font_candidates = ['Arial Unicode MS', 'Hiragino Sans GB', 'PingFang SC']
    else:  # Linux
        font_candidates = ['WenQuanYi Micro Hei', 'Noto Sans CJK SC', 'DejaVu Sans']
    
    # æ£€æŸ¥å¯ç”¨å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    chinese_font = None
    
    for font in font_candidates:
        if font in available_fonts:
            chinese_font = font
            break
    
    if chinese_font:
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        matplotlib.rcParams['font.sans-serif'] = [chinese_font, 'DejaVu Sans']
        matplotlib.rcParams['axes.unicode_minus'] = False
        matplotlib.rcParams['font.size'] = 10
        
        # å¼ºåˆ¶è®¾ç½®æ‰€æœ‰ç›¸å…³å‚æ•°
        matplotlib.rcParams['font.family'] = 'sans-serif'
        matplotlib.rcParams['axes.titlesize'] = 14
        matplotlib.rcParams['axes.labelsize'] = 12
        matplotlib.rcParams['legend.fontsize'] = 12
        
        # æµ‹è¯•ä¸­æ–‡æ˜¾ç¤º
        fig, ax = plt.subplots(figsize=(1, 1))
        ax.text(0.5, 0.5, 'æµ‹è¯•ä¸­æ–‡', fontsize=12, ha='center')
        ax.set_title('æµ‹è¯•æ ‡é¢˜')
        plt.close(fig)
        
        USE_CHINESE = True
        print(f"âœ“ æˆåŠŸè®¾ç½®ä¸­æ–‡å­—ä½“: {chinese_font}")
    else:
        raise Exception("æœªæ‰¾åˆ°å¯ç”¨çš„ä¸­æ–‡å­—ä½“")
        
except Exception as e:
    # å¦‚æœä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥ï¼Œä½¿ç”¨è‹±æ–‡
    matplotlib.rcParams['font.sans-serif'] = ['DejaVu Sans', 'Arial', 'Liberation Sans']
    matplotlib.rcParams['axes.unicode_minus'] = False
    matplotlib.rcParams['font.size'] = 10
    USE_CHINESE = False
    print(f"âš  ä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥: {str(e)}")
    print("å°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾æ˜¾ç¤ºå›¾è¡¨")

# é¢å¤–çš„å­—ä½“ç¼“å­˜æ¸…ç†
try:
    import matplotlib.pyplot as plt
    plt.rcdefaults()
    if USE_CHINESE and platform.system() == 'Windows':
        plt.rcParams['font.sans-serif'] = [chinese_font if 'chinese_font' in locals() else 'SimHei']
        plt.rcParams['axes.unicode_minus'] = False
except:
    pass

# ==================== è®­ç»ƒé…ç½®å‚æ•° ====================
# å¯ç›´æ¥ä¿®æ”¹çš„è®­ç»ƒå‚æ•°
# ä¿®æ”¹è®­ç»ƒé…ç½®å‚æ•°
TOTAL_EPOCHS = 100          # æ€»è®­ç»ƒè½®æ¬¡
BATCH_SIZE = 1              # æ‰¹æ¬¡å¤§å°
LEARNING_RATE = 0.001       # æ¢å¤åˆ°0.001ï¼Œé€šè¿‡é¢„çƒ­æœºåˆ¶æ§åˆ¶
WEIGHT_DECAY = 1e-4         # æƒé‡è¡°å‡
EARLY_STOP_PATIENCE = 25    # å¢åŠ æ—©åœè€å¿ƒå€¼
LR_SCHEDULER_PATIENCE = 12  # å¢åŠ å­¦ä¹ ç‡è°ƒåº¦è€å¿ƒå€¼
PRINT_FREQ = 10             # æ‰“å°é¢‘ç‡
WARMUP_EPOCHS = 5           # æ·»åŠ é¢„çƒ­è½®æ¬¡
MIN_LR = 1e-7               # æœ€å°å­¦ä¹ ç‡

# è·å–è„šæœ¬æ‰€åœ¨ç›®å½•
script_dir = os.path.dirname(os.path.abspath(__file__))

# ä½¿ç”¨ç»å¯¹è·¯å¾„åŠ è½½æ•°æ®
traindata = np.load(os.path.join(script_dir, "å¤„ç†å¥½çš„æ•°æ®é›†", "trainingdataset.npy"), allow_pickle=True)
testdata = np.load(os.path.join(script_dir, "å¤„ç†å¥½çš„æ•°æ®é›†", "testdataset.npy"), allow_pickle=True)

print(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(traindata)}")
print(f"æµ‹è¯•æ•°æ®é›†å¤§å°: {len(testdata)}")
print(f"æ€»è®­ç»ƒè½®æ¬¡: {TOTAL_EPOCHS}")
print(f"æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")

# æŸå¤±å‡½æ•°å®šä¹‰
class DiceLoss(nn.Module):
    def __init__(self, smooth=1):
        super(DiceLoss, self).__init__()
        self.smooth = smooth
    
    def forward(self, pred, target):
        pred = torch.sigmoid(pred)
        intersection = (pred * target).sum()
        dice = (2. * intersection + self.smooth) / (pred.sum() + target.sum() + self.smooth)
        return 1 - dice

class FocalLoss(nn.Module):
    def __init__(self, alpha=1, gamma=2):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
    
    def forward(self, pred, target):
        pred = torch.sigmoid(pred)
        ce_loss = F.binary_cross_entropy(pred, target, reduction='none')
        pt = torch.where(target == 1, pred, 1 - pred)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss
        return focal_loss.mean()

class CombinedLoss(nn.Module):
    def __init__(self, dice_weight=0.5, focal_weight=0.3, bce_weight=0.2):
        super(CombinedLoss, self).__init__()
        self.dice_weight = dice_weight
        self.focal_weight = focal_weight
        self.bce_weight = bce_weight
        self.dice_loss = DiceLoss()
        self.focal_loss = FocalLoss()
        self.bce_loss = nn.BCEWithLogitsLoss()
    
    def forward(self, pred, target):
        dice = self.dice_loss(pred, target)
        focal = self.focal_loss(pred, target)
        bce = self.bce_loss(pred, target)
        return self.dice_weight * dice + self.focal_weight * focal + self.bce_weight * bce

# æ•°æ®å¢å¼ºç±»
class DataAugmentation:
    def __init__(self):
        self.transforms = transforms.Compose([
            transforms.RandomHorizontalFlip(p=0.5),
            transforms.RandomVerticalFlip(p=0.5),
            transforms.RandomRotation(degrees=15),
        ])
    
    def __call__(self, image, mask):
        # åº”ç”¨ç›¸åŒçš„å˜æ¢åˆ°å›¾åƒå’Œæ©ç 
        seed = torch.randint(0, 2**32, (1,)).item()
        torch.manual_seed(seed)
        image = self.transforms(image)
        torch.manual_seed(seed)
        mask = self.transforms(mask)
        return image, mask

# æ•°æ®åº“åŠ è½½
class Dataset(Dataset):
    def __init__(self, data, augment=False):
        self.len = len(data)
        self.x_data = []
        self.y_data = []
        self.augment = augment
        self.augmentation = DataAugmentation() if augment else None
        
        for item in data:
            x, y = item[0], item[1]
            # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
            if isinstance(x, np.ndarray):
                x = torch.from_numpy(x.astype(np.float32))
            else:
                x = torch.tensor(x, dtype=torch.float32)
                
            if isinstance(y, np.ndarray):
                y = torch.from_numpy(y.astype(np.float32))
            else:
                y = torch.tensor(y, dtype=torch.float32)
                
            self.x_data.append(x)
            self.y_data.append(y)

    def __getitem__(self, index):
        x, y = self.x_data[index], self.y_data[index]
        
        # åº”ç”¨æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
        if self.augment and self.augmentation:
            x, y = self.augmentation(x, y)
            
        return x, y

    def __len__(self):
        return self.len

# æ•°æ®åº“dataloader
Train_dataset = Dataset(traindata, augment=True)  # è®­ç»ƒæ—¶ä½¿ç”¨æ•°æ®å¢å¼º
Test_dataset = Dataset(testdata, augment=False)   # æµ‹è¯•æ—¶ä¸ä½¿ç”¨æ•°æ®å¢å¼º
dataloader = DataLoader(Train_dataset, batch_size=BATCH_SIZE, shuffle=True)
testloader = DataLoader(Test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# è®­ç»ƒè®¾å¤‡é€‰æ‹©GPUè¿˜æ˜¯CPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æ¨¡å‹åˆå§‹åŒ–
model = UNet(1, 1)  # è¾“å…¥1é€šé“ï¼Œè¾“å‡º1é€šé“
model.to(device)

# æŸå¤±å‡½æ•°é€‰æ‹© - ä½¿ç”¨ç»„åˆæŸå¤±å‡½æ•°
criterion = CombinedLoss()
criterion.to(device)

# ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=LR_SCHEDULER_PATIENCE, verbose=True, min_lr=1e-6)

train_loss = []
test_loss = []

# è®­ç»ƒå‡½æ•°
# è®°å½•å®é™…å­¦ä¹ ç‡å˜åŒ–
lr_history = []  # æ·»åŠ å­¦ä¹ ç‡å†å²è®°å½•

def train(epoch, total_epochs):
    model.train()
    mloss = []
    train_bar = tqdm(dataloader, desc=f'ç¬¬{epoch:2d}/{total_epochs}è½®-è®­ç»ƒ', 
                     ncols=100, leave=True)
    
    for batch_idx, data in enumerate(train_bar):
        datavalue, datalabel = data
        datavalue, datalabel = datavalue.to(device), datalabel.to(device)
        
        optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­
        datalabel_pred = model(datavalue)
        loss = criterion(datalabel_pred, datalabel)
        
        # æ›´ä¸¥æ ¼çš„å¼‚å¸¸æ£€æŸ¥
        if torch.isnan(loss) or torch.isinf(loss) or loss.item() > 5.0:
            print(f"âš ï¸ æ£€æµ‹åˆ°å¼‚å¸¸æŸå¤±å€¼: {loss.item():.6f}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
            
        # åå‘ä¼ æ’­
        loss.backward()
        
        # è‡ªé€‚åº”æ¢¯åº¦è£å‰ª
        max_norm = 1.0 if epoch <= 10 else 0.5  # å‰10è½®ä½¿ç”¨è¾ƒå¤§çš„æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)
        
        # æ£€æŸ¥æ¢¯åº¦
        total_norm = 0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2
        total_norm = total_norm ** (1. / 2)
        
        if total_norm > 10.0:  # å¦‚æœæ¢¯åº¦è¿‡å¤§ï¼Œè·³è¿‡è¿™æ¬¡æ›´æ–°
            print(f"âš ï¸ æ¢¯åº¦è¿‡å¤§: {total_norm:.2f}ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
            continue
            
        optimizer.step()
        mloss.append(loss.item())
        
        train_bar.set_postfix({
            'Loss': f'{loss.item():.6f}',
            'Batch': f'{batch_idx+1}/{len(dataloader)}',
            'LR': f'{optimizer.param_groups[0]["lr"]:.2e}'
        })

    if len(mloss) == 0:  # å¦‚æœæ‰€æœ‰æ‰¹æ¬¡éƒ½è¢«è·³è¿‡
        print("âš ï¸ æ‰€æœ‰æ‰¹æ¬¡éƒ½è¢«è·³è¿‡ï¼Œä½¿ç”¨ä¸Šä¸€è½®æŸå¤±")
        epoch_train_loss = train_loss[-1] if train_loss else 1.0
    else:
        epoch_train_loss = np.mean(mloss)
    
    train_loss.append(epoch_train_loss)
    
    # è®°å½•å½“å‰å­¦ä¹ ç‡
    current_lr = optimizer.param_groups[0]['lr']
    lr_history.append(current_lr)
    
    print(f"ç¬¬{epoch:2d}è½® - è®­ç»ƒæŸå¤±: {epoch_train_loss:.6f}, å­¦ä¹ ç‡: {current_lr:.2e}")
    return epoch_train_loss

# æµ‹è¯•å‡½æ•°
def test(epoch, total_epochs):
    model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    mloss = []
    test_bar = tqdm(testloader, desc=f'ç¬¬{epoch:2d}/{total_epochs}è½®-æµ‹è¯•', 
                    ncols=100, leave=True)
    
    with torch.no_grad():
        for batch_idx, testdata in enumerate(test_bar):
            testdatavalue, testdatalabel = testdata
            testdatavalue, testdatalabel = testdatavalue.to(device), testdatalabel.to(device)
            testdatalabel_pred = model(testdatavalue)
            loss = criterion(testdatalabel_pred, testdatalabel)
            mloss.append(loss.item())
            
            # ç»Ÿä¸€è¿›åº¦æ¡åç¼€æ˜¾ç¤ºæ ¼å¼
            test_bar.set_postfix({
                'Loss': f'{loss.item():.6f}',
                'Batch': f'{batch_idx+1}/{len(testloader)}'
            })
            
    epoch_test_loss = torch.mean(torch.Tensor(mloss)).item()
    test_loss.append(epoch_test_loss)
    print(f"ç¬¬{epoch:2d}è½® - æµ‹è¯•æŸå¤±: {epoch_test_loss:.6f}")
    return epoch_test_loss

# æ—©åœæœºåˆ¶
class EarlyStopping:
    def __init__(self, patience=15, min_delta=1e-6):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        
    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience

bestmodel = None
bestepoch = None
bestloss = np.inf
early_stopping = EarlyStopping(patience=EARLY_STOP_PATIENCE)

# è®­ç»ƒå¾ªç¯
print("="*50)
print("å¼€å§‹è®­ç»ƒ...")
print("="*50)

# ä½¿ç”¨é…ç½®çš„æ€»è½®æ¬¡è¿›è¡Œè®­ç»ƒ
for epoch in tqdm(range(1, TOTAL_EPOCHS + 1), desc="æ€»ä½“è®­ç»ƒè¿›åº¦", ncols=100):
    train_loss_epoch = train(epoch, TOTAL_EPOCHS)
    test_loss_epoch = test(epoch, TOTAL_EPOCHS)
    
    # å­¦ä¹ ç‡è°ƒåº¦
    scheduler.step(test_loss_epoch)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if test_loss_epoch < bestloss:
        bestloss = test_loss_epoch
        bestepoch = epoch
        bestmodel = copy.deepcopy(model)
        print(f"âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ç¬¬ {epoch} è½®ï¼ŒæŸå¤±: {bestloss:.6f}")
    
    # æ—©åœæ£€æŸ¥
    if early_stopping(test_loss_epoch):
        print(f"\nâš  æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
        break
    
    # å®šæœŸæ‰“å°å½“å‰å­¦ä¹ ç‡
    if epoch % PRINT_FREQ == 0:
        current_lr = optimizer.param_groups[0]['lr']
        print(f"ğŸ“Š ç¬¬{epoch}è½® - å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
    
    print("-" * 50)

print("\n" + "="*50)
print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³è½®æ¬¡: {bestepoch}, æœ€ä½³æŸå¤±: {bestloss:.6f}")
print("="*50)

# ä¿å­˜æ¨¡å‹
print("æ­£åœ¨ä¿å­˜æ¨¡å‹...")
torch.save(model.state_dict(), os.path.join(script_dir, "è®­ç»ƒå¥½çš„æ¨¡å‹", "lastmodel.pt"))
if bestmodel is not None:
    torch.save(bestmodel.state_dict(), os.path.join(script_dir, "è®­ç»ƒå¥½çš„æ¨¡å‹", "bestmodel.pt"))
print("æ¨¡å‹ä¿å­˜å®Œæˆï¼")

# ä¿å­˜è®­ç»ƒç»“æœå›¾ï¼ˆæ ¹æ®å­—ä½“å¯ç”¨æ€§é€‰æ‹©è¯­è¨€ï¼‰
print("æ­£åœ¨ç”Ÿæˆè®­ç»ƒç»“æœå›¾...")
plt.figure(figsize=(14, 10))

# æ ¹æ®å­—ä½“å¯ç”¨æ€§è®¾ç½®æ ‡ç­¾
if USE_CHINESE:
    train_label = 'è®­ç»ƒæŸå¤±'
    test_label = 'æµ‹è¯•æŸå¤±'
    xlabel_1 = 'è®­ç»ƒè½®æ¬¡'
    ylabel_1 = 'æŸå¤±å€¼'
    title_1 = 'è®­ç»ƒå’Œæµ‹è¯•æŸå¤±æ›²çº¿'
    best_label = f'æœ€ä½³è½®æ¬¡: {bestepoch}' if bestepoch else ''
    lr_label = 'å­¦ä¹ ç‡'
    xlabel_2 = 'è®­ç»ƒè½®æ¬¡'
    ylabel_2 = 'å­¦ä¹ ç‡'
    title_2 = 'å­¦ä¹ ç‡è°ƒåº¦æ›²çº¿'
else:
    train_label = 'Training Loss'
    test_label = 'Validation Loss'
    xlabel_1 = 'Epoch'
    ylabel_1 = 'Loss'
    title_1 = 'Training and Validation Loss Curves'
    best_label = f'Best Epoch: {bestepoch}' if bestepoch else ''
    lr_label = 'Learning Rate'
    xlabel_2 = 'Epoch'
    ylabel_2 = 'Learning Rate'
    title_2 = 'Learning Rate Schedule'

# ç¬¬ä¸€ä¸ªå­å›¾ï¼šæŸå¤±æ›²çº¿
plt.subplot(2, 1, 1)
plt.plot(range(1, len(train_loss) + 1), train_loss, label=train_label, color='blue', linewidth=2)
plt.plot(range(1, len(test_loss) + 1), test_loss, label=test_label, color='red', linewidth=2)
plt.legend(fontsize=12)
plt.xlabel(xlabel_1, fontsize=12)
plt.ylabel(ylabel_1, fontsize=12)
plt.title(title_1, fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# æ ‡è®°æœ€ä½³ç‚¹
if bestepoch is not None:
    plt.axvline(x=bestepoch, color='green', linestyle='--', alpha=0.7, label=best_label)
    plt.legend(fontsize=12)

# ç¬¬äºŒä¸ªå­å›¾ï¼šå­¦ä¹ ç‡æ›²çº¿
plt.subplot(2, 1, 2)
if len(lr_history) > 0:
    plt.plot(range(1, len(lr_history) + 1), lr_history, label=lr_label, color='orange', linewidth=2)
else:
    # å¤‡ç”¨æ˜¾ç¤º
    lr_values = [LEARNING_RATE] * len(train_loss)
    plt.plot(range(1, len(lr_values) + 1), lr_values, label=lr_label, color='orange', linewidth=2)

plt.xlabel(xlabel_2, fontsize=12)
plt.ylabel(ylabel_2, fontsize=12)
plt.title(title_2, fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)
plt.yscale('log')
plt.legend(fontsize=12)

# æ·»åŠ æŸå¤±èŒƒå›´é™åˆ¶æ˜¾ç¤º
plt.subplot(2, 1, 1)
# é™åˆ¶yè½´èŒƒå›´ä»¥æ›´å¥½åœ°æ˜¾ç¤ºæ­£å¸¸æŸå¤±å˜åŒ–
max_loss = max(max(train_loss), max(test_loss))
if max_loss > 5.0:  # å¦‚æœæœ€å¤§æŸå¤±è¿‡å¤§ï¼Œé™åˆ¶æ˜¾ç¤ºèŒƒå›´
    plt.ylim(0, min(max_loss, 2.0))
    plt.text(0.02, 0.98, f'æ³¨æ„ï¼šéƒ¨åˆ†æŸå¤±å€¼è¶…å‡ºæ˜¾ç¤ºèŒƒå›´\næœ€å¤§æŸå¤±: {max_loss:.2f}', 
             transform=plt.gca().transAxes, fontsize=10, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.7))

plt.plot(range(1, len(train_loss) + 1), train_loss, label=train_label, color='blue', linewidth=2)
plt.plot(range(1, len(test_loss) + 1), test_loss, label=test_label, color='red', linewidth=2)
plt.legend(fontsize=12)
plt.xlabel(xlabel_1, fontsize=12)
plt.ylabel(ylabel_1, fontsize=12)
plt.title(title_1, fontsize=14, fontweight='bold')
plt.grid(True, alpha=0.3)

# ä¿å­˜å›¾ç‰‡
save_path = os.path.join(script_dir, 'è®­ç»ƒå¥½çš„æ¨¡å‹', 'training_results.png')
plt.tight_layout()  # è°ƒæ•´å¸ƒå±€
plt.savefig(save_path, dpi=300, bbox_inches='tight')  # ä¿å­˜é«˜è´¨é‡å›¾ç‰‡

plt.show()

print("è®­ç»ƒç»“æœå›¾ç”Ÿæˆå®Œæˆï¼")
print(f"å›¾ç‰‡ä¿å­˜ä½ç½®: {save_path}")
print("="*50)
print("è®­ç»ƒå®Œæˆï¼")
